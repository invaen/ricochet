---
phase: 05-crawler-payloads
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - ricochet/injection/crawler.py
  - ricochet/injection/__init__.py
  - ricochet/cli.py
autonomous: true

must_haves:
  truths:
    - "User can run ricochet crawl -u URL and see discovered pages"
    - "Crawler extracts links from HTML anchor tags"
    - "Crawler extracts forms with their input fields"
    - "Crawler respects same-domain filtering"
    - "Crawler respects depth and page limits"
  artifacts:
    - path: "ricochet/injection/crawler.py"
      provides: "LinkFormExtractor, FormData, Crawler class"
      exports: ["LinkFormExtractor", "FormData", "ExtractedData", "CrawlResult", "Crawler"]
    - path: "ricochet/cli.py"
      provides: "crawl subcommand"
      contains: "cmd_crawl"
  key_links:
    - from: "ricochet/cli.py"
      to: "ricochet/injection/crawler.py"
      via: "import Crawler"
      pattern: "from ricochet\\.injection\\.crawler import"
    - from: "ricochet/injection/crawler.py"
      to: "ricochet/injection/http_client.py"
      via: "uses send_request for fetching"
      pattern: "from ricochet\\.injection\\.http_client import"
---

<objective>
Implement web crawler for automatic injection point discovery

Purpose: Enable users to auto-discover injection points (forms, URL parameters) by crawling a target site, eliminating manual enumeration of endpoints.

Output: Working `ricochet crawl -u URL` command that discovers and displays injection points from crawled pages.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-crawler-payloads/05-RESEARCH.md

# Existing injection infrastructure
@ricochet/injection/__init__.py
@ricochet/injection/http_client.py
@ricochet/injection/vectors.py
@ricochet/cli.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create crawler module with HTMLParser-based extraction</name>
  <files>ricochet/injection/crawler.py</files>
  <action>
Create `ricochet/injection/crawler.py` with:

1. **Data classes:**
   - `FormData(action: str, method: str, inputs: list[tuple[str, str, str]])` - form action, method, list of (name, type, value)
   - `ExtractedData(links: list[str], forms: list[FormData])` - extracted content from a page
   - `CrawlResult(url: str, depth: int, forms: list[FormData], links: list[str], error: Optional[str])` - result per page

2. **LinkFormExtractor(HTMLParser):**
   - `__init__`: Initialize result, _current_form
   - `handle_starttag`: Extract href from `<a>`, start form from `<form>`, collect inputs from `<input>/<select>/<textarea>`
   - `handle_endtag`: Close form on `</form>`
   - `extract(html: str) -> ExtractedData`: Reset state, feed html, return result

3. **URL helper functions:**
   - `normalize_url(base_url: str, href: str) -> Optional[str]`: Use urljoin, urldefrag; skip javascript:/mailto:/tel:/data:; validate http/https scheme
   - `is_same_domain(base_url: str, target_url: str) -> bool`: Compare netloc via urlparse
   - `is_crawlable_url(url: str) -> bool`: Skip binary extensions (.pdf, .jpg, .png, .gif, .svg, .css, .js, .ico, .woff, .zip, etc.)

4. **Crawler class:**
   - `__init__(max_depth: int = 2, max_pages: int = 100, timeout: float = 10.0, rate_limit: float = 10.0)`: Store config, create RateLimiter
   - `crawl(seed_url: str) -> list[CrawlResult]`: BFS using deque, track visited set, respect limits
   - `_process_page(url: str, depth: int) -> CrawlResult`: Fetch via send_request, extract links/forms, handle errors

Use imports from existing modules:
- `from ricochet.injection.http_client import send_request`
- `from ricochet.injection.rate_limiter import RateLimiter`
- `from html.parser import HTMLParser`
- `from urllib.parse import urljoin, urlparse, urldefrag`
- `from collections import deque`
- `from dataclasses import dataclass, field`
  </action>
  <verify>
```bash
cd /Users/aidan/projects/security/ricochet
python -c "from ricochet.injection.crawler import Crawler, LinkFormExtractor, FormData, ExtractedData, CrawlResult; print('Imports OK')"
```
  </verify>
  <done>Crawler module exists with all classes importable, no syntax errors</done>
</task>

<task type="auto">
  <name>Task 2: Add crawl subcommand to CLI</name>
  <files>ricochet/cli.py, ricochet/injection/__init__.py</files>
  <action>
1. **Update ricochet/injection/__init__.py:**
   - Add imports: `from ricochet.injection.crawler import Crawler, CrawlResult, FormData, ExtractedData, LinkFormExtractor`
   - Add to `__all__`: "Crawler", "CrawlResult", "FormData", "ExtractedData", "LinkFormExtractor"

2. **Update ricochet/cli.py create_parser():**
   Add `crawl` subcommand after inject_parser:
   ```python
   crawl_parser = subparsers.add_parser(
       'crawl',
       help='Crawl target to discover injection points'
   )
   crawl_parser.add_argument(
       '-u', '--url',
       required=True,
       help='Starting URL for crawl'
   )
   crawl_parser.add_argument(
       '--depth',
       type=int,
       default=2,
       help='Maximum crawl depth (default: 2)'
   )
   crawl_parser.add_argument(
       '--max-pages',
       type=int,
       default=100,
       help='Maximum pages to crawl (default: 100)'
   )
   crawl_parser.add_argument(
       '--timeout',
       type=float,
       default=10.0,
       help='Request timeout in seconds (default: 10)'
   )
   crawl_parser.add_argument(
       '--rate',
       type=float,
       default=10.0,
       help='Requests per second (default: 10)'
   )
   crawl_parser.set_defaults(func=cmd_crawl)
   ```

3. **Add cmd_crawl function:**
   ```python
   def cmd_crawl(args, store) -> int:
       """Handle crawl subcommand - discover injection points."""
       from ricochet.injection.crawler import Crawler
       from urllib.parse import urlparse

       # Validate URL
       parsed = urlparse(args.url)
       if not parsed.scheme:
           args.url = f"http://{args.url}"
       if not urlparse(args.url).netloc:
           print(f"Error: Invalid URL: {args.url}", file=sys.stderr)
           return 2

       crawler = Crawler(
           max_depth=args.depth,
           max_pages=args.max_pages,
           timeout=args.timeout,
           rate_limit=args.rate,
       )

       print(f"Crawling {args.url} (depth={args.depth}, max_pages={args.max_pages})")
       print()

       results = crawler.crawl(args.url)

       # Display results
       total_forms = 0
       total_links = 0

       for result in results:
           if result.error:
               print(f"[-] {result.url}")
               print(f"    Error: {result.error}")
           else:
               print(f"[+] {result.url} (depth={result.depth})")
               if result.forms:
                   for form in result.forms:
                       total_forms += 1
                       inputs = ", ".join(f"{n}({t})" for n, t, v in form.inputs if n)
                       print(f"    Form: {form.method} {form.action or '(same page)'}")
                       print(f"      Inputs: {inputs or '(none)'}")
               print(f"    Links: {len(result.links)}")
               total_links += len(result.links)
           print()

       # Summary
       print(f"=== Summary ===")
       print(f"Pages crawled: {len(results)}")
       print(f"Forms found: {total_forms}")
       print(f"Links discovered: {total_links}")

       return 0
   ```
  </action>
  <verify>
```bash
cd /Users/aidan/projects/security/ricochet
python -m ricochet crawl --help
```
Expected: Shows crawl command help with -u, --depth, --max-pages, --timeout, --rate options
  </verify>
  <done>crawl subcommand shows in help with all options documented</done>
</task>

<task type="auto">
  <name>Task 3: End-to-end crawl verification</name>
  <files></files>
  <action>
Test the crawler against a simple test case:

1. Create a minimal test HTML file to verify parsing:
```bash
cd /Users/aidan/projects/security/ricochet
python -c "
from ricochet.injection.crawler import LinkFormExtractor

html = '''
<html>
<body>
  <a href=\"/page1\">Link 1</a>
  <a href=\"/page2?q=test\">Link 2</a>
  <form action=\"/login\" method=\"POST\">
    <input type=\"text\" name=\"username\">
    <input type=\"password\" name=\"password\">
    <input type=\"submit\" value=\"Login\">
  </form>
</body>
</html>
'''

extractor = LinkFormExtractor()
result = extractor.extract(html)
print(f'Links: {result.links}')
print(f'Forms: {len(result.forms)}')
for form in result.forms:
    print(f'  {form.method} {form.action}: {[(n, t) for n, t, v in form.inputs]}')
"
```

2. Test URL normalization:
```bash
python -c "
from ricochet.injection.crawler import normalize_url, is_same_domain, is_crawlable_url

base = 'http://example.com/dir/page.html'

# Test relative URL resolution
print(f'Relative: {normalize_url(base, \"other.html\")}')
print(f'Absolute path: {normalize_url(base, \"/root.html\")}')
print(f'Skip javascript: {normalize_url(base, \"javascript:void(0)\")}')
print(f'Skip fragment only: {normalize_url(base, \"#anchor\")}')

# Test domain filtering
print(f'Same domain: {is_same_domain(base, \"http://example.com/other\")}')
print(f'Different domain: {is_same_domain(base, \"http://other.com/page\")}')

# Test crawlable URL
print(f'HTML crawlable: {is_crawlable_url(\"http://x.com/page.html\")}')
print(f'PDF not crawlable: {is_crawlable_url(\"http://x.com/doc.pdf\")}')
"
```

3. Verify CLI works (will fail to connect but should show proper error handling):
```bash
python -m ricochet crawl -u http://127.0.0.1:9999 --max-pages 1 --depth 0
```
Expected: Shows crawl attempt with connection error (no server running)
  </action>
  <verify>
```bash
cd /Users/aidan/projects/security/ricochet
python -c "
from ricochet.injection.crawler import LinkFormExtractor
html = '<a href=\"/test\">Link</a><form action=\"/submit\"><input name=\"q\"></form>'
r = LinkFormExtractor().extract(html)
assert r.links == ['/test'], f'Links wrong: {r.links}'
assert len(r.forms) == 1, f'Forms wrong: {len(r.forms)}'
assert r.forms[0].inputs[0][0] == 'q', f'Input name wrong'
print('All assertions passed')
"
```
  </verify>
  <done>LinkFormExtractor correctly parses HTML, extracting links and forms with inputs</done>
</task>

</tasks>

<verification>
1. `python -m ricochet crawl --help` shows crawl command with all options
2. `python -c "from ricochet.injection.crawler import Crawler, LinkFormExtractor"` imports without error
3. LinkFormExtractor correctly parses sample HTML with links and forms
4. URL helpers correctly normalize, filter domains, and detect binary files
</verification>

<success_criteria>
- User can run `ricochet crawl -u URL` and see discovered pages/forms
- Crawler discovers forms with input fields from HTML
- Crawler extracts and follows links within same domain
- Crawler respects --depth and --max-pages limits
- All crawler classes exported from ricochet.injection module
</success_criteria>

<output>
After completion, create `.planning/phases/05-crawler-payloads/05-01-SUMMARY.md`
</output>
