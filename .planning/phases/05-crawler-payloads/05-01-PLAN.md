---
phase: 05-crawler-payloads
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - ricochet/injection/crawler.py
  - ricochet/injection/__init__.py
  - ricochet/cli.py
autonomous: true

must_haves:
  truths:
    - "User can run ricochet crawl -u URL and see discovered pages"
    - "Crawler extracts links from HTML anchor tags"
    - "Crawler extracts forms with their input fields"
    - "Crawler respects same-domain filtering"
    - "Crawler respects depth and page limits"
    - "User can export crawl results with --export vectors.json"
    - "User can inject using crawl results with ricochet inject --from-crawl vectors.json"
  artifacts:
    - path: "ricochet/injection/crawler.py"
      provides: "LinkFormExtractor, FormData, Crawler class, CrawlVector, export_vectors, load_crawl_vectors"
      exports: ["LinkFormExtractor", "FormData", "ExtractedData", "CrawlResult", "Crawler", "CrawlVector", "export_vectors", "load_crawl_vectors"]
    - path: "ricochet/cli.py"
      provides: "crawl subcommand with --export, inject --from-crawl"
      contains: "cmd_crawl"
  key_links:
    - from: "ricochet/cli.py"
      to: "ricochet/injection/crawler.py"
      via: "import Crawler, export_vectors, load_crawl_vectors"
      pattern: "from ricochet\\.injection\\.crawler import"
    - from: "ricochet/injection/crawler.py"
      to: "ricochet/injection/http_client.py"
      via: "uses send_request for fetching"
      pattern: "from ricochet\\.injection\\.http_client import"
    - from: "ricochet/cli.py cmd_inject"
      to: "ricochet/injection/crawler.py"
      via: "load_crawl_vectors for --from-crawl"
      pattern: "load_crawl_vectors.*args\\.from_crawl"
---

<objective>
Implement web crawler for automatic injection point discovery with export/import for inject workflow

Purpose: Enable users to auto-discover injection points (forms, URL parameters) by crawling a target site, and seamlessly use those discovered vectors with the inject command.

Output: Working `ricochet crawl -u URL --export vectors.json` that discovers injection points, and `ricochet inject --from-crawl vectors.json` that injects into discovered vectors.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-crawler-payloads/05-RESEARCH.md

# Existing injection infrastructure
@ricochet/injection/__init__.py
@ricochet/injection/http_client.py
@ricochet/injection/vectors.py
@ricochet/cli.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create crawler module with HTMLParser-based extraction and export support</name>
  <files>ricochet/injection/crawler.py</files>
  <action>
Create `ricochet/injection/crawler.py` with:

1. **Data classes:**
   - `FormData(action: str, method: str, inputs: list[tuple[str, str, str]])` - form action, method, list of (name, type, value)
   - `ExtractedData(links: list[str], forms: list[FormData])` - extracted content from a page
   - `CrawlResult(url: str, depth: int, forms: list[FormData], links: list[str], error: Optional[str])` - result per page
   - `CrawlVector(url: str, method: str, param_name: str, param_type: str, location: str)` - exportable injection vector (location is 'form', 'query', 'body')

2. **LinkFormExtractor(HTMLParser):**
   - `__init__`: Initialize result, _current_form
   - `handle_starttag`: Extract href from `<a>`, start form from `<form>`, collect inputs from `<input>/<select>/<textarea>`
   - `handle_endtag`: Close form on `</form>`
   - `extract(html: str) -> ExtractedData`: Reset state, feed html, return result

3. **URL helper functions:**
   - `normalize_url(base_url: str, href: str) -> Optional[str]`: Use urljoin, urldefrag; skip javascript:/mailto:/tel:/data:; validate http/https scheme
   - `is_same_domain(base_url: str, target_url: str) -> bool`: Compare netloc via urlparse
   - `is_crawlable_url(url: str) -> bool`: Skip binary extensions (.pdf, .jpg, .png, .gif, .svg, .css, .js, .ico, .woff, .zip, etc.)

4. **Crawler class:**
   - `__init__(max_depth: int = 2, max_pages: int = 100, timeout: float = 10.0, rate_limit: float = 10.0)`: Store config, create RateLimiter
   - `crawl(seed_url: str) -> list[CrawlResult]`: BFS using deque, track visited set, respect limits
   - `_process_page(url: str, depth: int) -> CrawlResult`: Fetch via send_request, extract links/forms, handle errors

5. **Export/import functions for inject workflow integration:**
   - `results_to_vectors(results: list[CrawlResult]) -> list[CrawlVector]`: Convert crawl results to injectable vectors
     - For each form: create CrawlVector for each input with name, method=form.method, url=resolved form action, location='form'
     - For each URL with query params: create CrawlVector for each param, method='GET', location='query'
   - `export_vectors(vectors: list[CrawlVector], filepath: Path) -> None`: Write vectors to JSON file
   - `load_crawl_vectors(filepath: Path) -> list[CrawlVector]`: Load vectors from JSON file

Use imports from existing modules:
- `from ricochet.injection.http_client import send_request`
- `from ricochet.injection.rate_limiter import RateLimiter`
- `from html.parser import HTMLParser`
- `from urllib.parse import urljoin, urlparse, urldefrag, parse_qs`
- `from collections import deque`
- `from dataclasses import dataclass, field, asdict`
- `from pathlib import Path`
- `import json`
  </action>
  <verify>
```bash
cd /Users/aidan/projects/security/ricochet
python -c "from ricochet.injection.crawler import Crawler, LinkFormExtractor, FormData, ExtractedData, CrawlResult, CrawlVector, export_vectors, load_crawl_vectors, results_to_vectors; print('Imports OK')"
```
  </verify>
  <done>Crawler module exists with all classes and export/import functions importable, no syntax errors</done>
</task>

<task type="auto">
  <name>Task 2: Add crawl subcommand with --export and inject --from-crawl flag</name>
  <files>ricochet/cli.py, ricochet/injection/__init__.py</files>
  <action>
1. **Update ricochet/injection/__init__.py:**
   - Add imports: `from ricochet.injection.crawler import Crawler, CrawlResult, FormData, ExtractedData, LinkFormExtractor, CrawlVector, export_vectors, load_crawl_vectors, results_to_vectors`
   - Add to `__all__`: "Crawler", "CrawlResult", "FormData", "ExtractedData", "LinkFormExtractor", "CrawlVector", "export_vectors", "load_crawl_vectors", "results_to_vectors"

2. **Update ricochet/cli.py create_parser() - add crawl subcommand:**
   Add `crawl` subcommand after inject_parser:
   ```python
   crawl_parser = subparsers.add_parser(
       'crawl',
       help='Crawl target to discover injection points'
   )
   crawl_parser.add_argument(
       '-u', '--url',
       required=True,
       help='Starting URL for crawl'
   )
   crawl_parser.add_argument(
       '--depth',
       type=int,
       default=2,
       help='Maximum crawl depth (default: 2)'
   )
   crawl_parser.add_argument(
       '--max-pages',
       type=int,
       default=100,
       help='Maximum pages to crawl (default: 100)'
   )
   crawl_parser.add_argument(
       '--timeout',
       type=float,
       default=10.0,
       help='Request timeout in seconds (default: 10)'
   )
   crawl_parser.add_argument(
       '--rate',
       type=float,
       default=10.0,
       help='Requests per second (default: 10)'
   )
   crawl_parser.add_argument(
       '--export',
       type=Path,
       help='Export discovered vectors to JSON file for use with inject --from-crawl'
   )
   crawl_parser.set_defaults(func=cmd_crawl)
   ```

3. **Update inject_parser - add --from-crawl flag:**
   Add after existing --payload argument:
   ```python
   inject_parser.add_argument(
       '--from-crawl',
       type=Path,
       help='Load injection targets from crawl export file (use with ricochet crawl --export)'
   )
   ```

4. **Add cmd_crawl function:**
   ```python
   def cmd_crawl(args, store) -> int:
       """Handle crawl subcommand - discover injection points."""
       from ricochet.injection.crawler import Crawler, export_vectors, results_to_vectors
       from urllib.parse import urlparse

       # Validate URL
       parsed = urlparse(args.url)
       if not parsed.scheme:
           args.url = f"http://{args.url}"
       if not urlparse(args.url).netloc:
           print(f"Error: Invalid URL: {args.url}", file=sys.stderr)
           return 2

       crawler = Crawler(
           max_depth=args.depth,
           max_pages=args.max_pages,
           timeout=args.timeout,
           rate_limit=args.rate,
       )

       print(f"Crawling {args.url} (depth={args.depth}, max_pages={args.max_pages})")
       print()

       results = crawler.crawl(args.url)

       # Display results
       total_forms = 0
       total_links = 0

       for result in results:
           if result.error:
               print(f"[-] {result.url}")
               print(f"    Error: {result.error}")
           else:
               print(f"[+] {result.url} (depth={result.depth})")
               if result.forms:
                   for form in result.forms:
                       total_forms += 1
                       inputs = ", ".join(f"{n}({t})" for n, t, v in form.inputs if n)
                       print(f"    Form: {form.method} {form.action or '(same page)'}")
                       print(f"      Inputs: {inputs or '(none)'}")
               print(f"    Links: {len(result.links)}")
               total_links += len(result.links)
           print()

       # Summary
       print(f"=== Summary ===")
       print(f"Pages crawled: {len(results)}")
       print(f"Forms found: {total_forms}")
       print(f"Links discovered: {total_links}")

       # Export if requested
       if args.export:
           vectors = results_to_vectors(results)
           export_vectors(vectors, args.export)
           print()
           print(f"Exported {len(vectors)} injection vectors to {args.export}")
           print(f"Use with: ricochet inject --from-crawl {args.export} --payload '{{{{CALLBACK}}}}'")

       return 0
   ```

5. **Update cmd_inject to support --from-crawl:**
   At the start of cmd_inject, after imports, add handling for --from-crawl:
   ```python
   # Handle --from-crawl mode
   if args.from_crawl:
       from ricochet.injection.crawler import load_crawl_vectors, CrawlVector

       if not args.from_crawl.exists():
           print(f"Error: Crawl export file not found: {args.from_crawl}", file=sys.stderr)
           return 2

       try:
           crawl_vectors = load_crawl_vectors(args.from_crawl)
       except (json.JSONDecodeError, KeyError) as e:
           print(f"Error: Invalid crawl export file: {e}", file=sys.stderr)
           return 2

       if not crawl_vectors:
           print("Warning: No vectors found in crawl export file", file=sys.stderr)
           return 0

       print(f"Loaded {len(crawl_vectors)} vectors from {args.from_crawl}")
       print()

       # Process each crawl vector
       all_results = []
       for cv in crawl_vectors:
           # Build minimal request for this vector
           parsed_url = urlparse(cv.url)
           path = parsed_url.path or "/"
           if parsed_url.query:
               path = f"{path}?{parsed_url.query}"

           request = ParsedRequest(
               method=cv.method,
               path=path,
               http_version="HTTP/1.1",
               headers={"Host": parsed_url.netloc, "User-Agent": "Ricochet/1.0"},
               body=None,
               host=parsed_url.netloc,
           )
           use_https = parsed_url.scheme == "https"

           result = injector.inject_single_param(
               request=request,
               param_name=cv.param_name,
               payload=args.payload,
               use_https=use_https,
               dry_run=args.dry_run,
           )
           if result:
               all_results.append(result)

       results = all_results
       # Continue to display results section...
   ```

   Also add `import json` at top of cmd_inject if not already present.
  </action>
  <verify>
```bash
cd /Users/aidan/projects/security/ricochet
python -m ricochet crawl --help | grep -E "(export|from-crawl)"
python -m ricochet inject --help | grep -E "from-crawl"
```
Expected: Shows --export option in crawl help, --from-crawl option in inject help
  </verify>
  <done>crawl subcommand shows --export flag, inject subcommand shows --from-crawl flag</done>
</task>

<task type="auto">
  <name>Task 3: Verify crawler parsing and URL helpers</name>
  <files></files>
  <action>
Test the crawler parsing components:

1. Create a minimal test HTML file to verify parsing:
```bash
cd /Users/aidan/projects/security/ricochet
python -c "
from ricochet.injection.crawler import LinkFormExtractor

html = '''
<html>
<body>
  <a href=\"/page1\">Link 1</a>
  <a href=\"/page2?q=test\">Link 2</a>
  <form action=\"/login\" method=\"POST\">
    <input type=\"text\" name=\"username\">
    <input type=\"password\" name=\"password\">
    <input type=\"submit\" value=\"Login\">
  </form>
</body>
</html>
'''

extractor = LinkFormExtractor()
result = extractor.extract(html)
print(f'Links: {result.links}')
print(f'Forms: {len(result.forms)}')
for form in result.forms:
    print(f'  {form.method} {form.action}: {[(n, t) for n, t, v in form.inputs]}')
"
```

2. Test URL normalization:
```bash
python -c "
from ricochet.injection.crawler import normalize_url, is_same_domain, is_crawlable_url

base = 'http://example.com/dir/page.html'

# Test relative URL resolution
print(f'Relative: {normalize_url(base, \"other.html\")}')
print(f'Absolute path: {normalize_url(base, \"/root.html\")}')
print(f'Skip javascript: {normalize_url(base, \"javascript:void(0)\")}')
print(f'Skip fragment only: {normalize_url(base, \"#anchor\")}')

# Test domain filtering
print(f'Same domain: {is_same_domain(base, \"http://example.com/other\")}')
print(f'Different domain: {is_same_domain(base, \"http://other.com/page\")}')

# Test crawlable URL
print(f'HTML crawlable: {is_crawlable_url(\"http://x.com/page.html\")}')
print(f'PDF not crawlable: {is_crawlable_url(\"http://x.com/doc.pdf\")}')
"
```

3. Verify CLI works (will fail to connect but should show proper error handling):
```bash
python -m ricochet crawl -u http://127.0.0.1:9999 --max-pages 1 --depth 0
```
Expected: Shows crawl attempt with connection error (no server running)
  </action>
  <verify>
```bash
cd /Users/aidan/projects/security/ricochet
python -c "
from ricochet.injection.crawler import LinkFormExtractor
html = '<a href=\"/test\">Link</a><form action=\"/submit\"><input name=\"q\"></form>'
r = LinkFormExtractor().extract(html)
assert r.links == ['/test'], f'Links wrong: {r.links}'
assert len(r.forms) == 1, f'Forms wrong: {len(r.forms)}'
assert r.forms[0].inputs[0][0] == 'q', f'Input name wrong'
print('All assertions passed')
"
```
  </verify>
  <done>LinkFormExtractor correctly parses HTML, extracting links and forms with inputs</done>
</task>

</tasks>

<verification>
1. `python -m ricochet crawl --help` shows crawl command with all options including --export
2. `python -m ricochet inject --help` shows --from-crawl option
3. `python -c "from ricochet.injection.crawler import Crawler, LinkFormExtractor, export_vectors, load_crawl_vectors"` imports without error
4. LinkFormExtractor correctly parses sample HTML with links and forms
5. URL helpers correctly normalize, filter domains, and detect binary files
6. Crawl --export creates valid JSON file
7. Inject --from-crawl loads vectors and injects
</verification>

<success_criteria>
- User can run `ricochet crawl -u URL` and see discovered pages/forms
- User can run `ricochet crawl -u URL --export vectors.json` to export vectors
- User can run `ricochet inject --from-crawl vectors.json --payload X` to inject into crawled vectors
- Crawler discovers forms with input fields from HTML
- Crawler extracts and follows links within same domain
- Crawler respects --depth and --max-pages limits
- All crawler classes exported from ricochet.injection module
</success_criteria>

<output>
After completion, create `.planning/phases/05-crawler-payloads/05-01-SUMMARY.md`
</output>
